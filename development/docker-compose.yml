version: '3.8'

x-logging:
  &logging
  logging:
    driver: loki
    options:
      loki-url: http://172.22.22.15:3100/loki/api/v1/push
      loki-retries: 5
      loki-batch-size: 400

x-airflow-common:
  &airflow-common
  image: ghcr.io/togethercrew/airflow-dags:main
  env_file:
    &airflow-common-env
    - ./.env.airflow
  volumes:
    - airflow_logs:/opt/airflow/logs
    - airflow_config:/opt/airflow/config
    - airflow_plugins:/opt/airflow/plugins
  user: "50000:0"
  depends_on:
    &airflow-common-depends-on
    airflow-redis:
      condition: service_healthy
    pgvector:
      condition: service_healthy
    neo4j-dev:
      condition: service_healthy
  networks:
    - development
    - monitoring
  <<: *logging

x-redis-common:
  &redis-common
  image: redis:7.0.11
  restart: unless-stopped
  command: [ "redis-server", "/usr/local/etc/redis/redis.conf" ]
  healthcheck:
    test: [ "CMD", "redis-cli", "--raw", "incr", "ping" ]
    interval: 60s
    timeout: 10s
    retries: 2
    start_period: 40s
  <<: *logging

x-hivemind-common:
  &hivemind-common
  image: ghcr.io/togethercrew/hivemind-bot:main
  restart: unless-stopped
  env_file:
    - ./.env.hivemind-bot
  depends_on:
    neo4j-dev:
      condition: service_healthy
    mongodb-dev:
      condition: service_healthy
    pgvector:
      condition: service_healthy
  networks:
    - development
    - monitoring
  <<: *logging

services:
  mongodb-dev:
    image: mongo:7
    restart: unless-stopped
    command: ['--replSet', 'rs0', '--keyFile', '/etc/mongo/replica.key']
    ports:
      - 37017:27017
    env_file:
      - ./.env.db
    volumes:
      - ../init-mongo.sh:/docker-entrypoint-initdb.d/init-mongo.sh:ro
      - ./mongo:/etc/mongo:ro
      - mongodb_data_container:/data/db
    healthcheck:
      test: test $(echo $(mongosh -u $$MONGO_INITDB_ROOT_USERNAME -p $$MONGO_INITDB_ROOT_PASSWORD --quiet /etc/mongo/healthcheck.js)) -eq 1
      interval: 60s
      timeout: 10s
      retries: 2
      start_period: 40s
    networks:
      - development
      - monitoring
    <<: *logging

  rabbitmq-dev:
    image: rabbitmq:3-management
    restart: unless-stopped
    ports:
      - 6672:5672
      - 25672:15672
    volumes:
      - rmq_data_container:/var/lib/rabbitmq/
      - ./rabbitmq/enabled_plugins:/etc/rabbitmq/enabled_plugins
    env_file:
      - ./.env.rmq
    healthcheck:
      test: rabbitmq-diagnostics -q ping
      interval: 30s
      timeout: 30s
      retries: 2
      start_period: 40s
    networks:
      - development
      - monitoring
    <<: *logging

  redis-discord-bot-dev:
    image: redis:7.0.11
    restart: unless-stopped
    command: [ "redis-server", "/usr/local/etc/redis/redis.conf" ]
    volumes:
      - type: bind
        source: ./redis/discordBot.conf
        target: /usr/local/etc/redis/redis.conf
        read_only: true
    healthcheck:
      test: [ "CMD", "redis-cli", "--raw", "incr", "ping" ]
      interval: 60s
      timeout: 10s
      retries: 2
      start_period: 40s
    networks:
      - development
      - monitoring
    <<: *logging

  redis-discord-analyzer-dev:
    image: redis:7.0.11
    restart: unless-stopped
    command: [ "redis-server", "/usr/local/etc/redis/redis.conf" ]
    volumes:
      - type: bind
        source: ./redis/discordAnalyzer.conf
        target: /usr/local/etc/redis/redis.conf
        read_only: true
    healthcheck:
      test: [ "CMD", "redis-cli", "--raw", "incr", "ping" ]
      interval: 60s
      timeout: 10s
      retries: 2
      start_period: 40s
    networks:
      - development
      - monitoring
    <<: *logging

  neo4j-dev:
    image: neo4j:5.9.0
    restart: unless-stopped
    ports:
      - 27474:7474
      - 27687:7687
    env_file:
      - ./.env.neo4j
    volumes:
      # - neo4j_conf:/var/lib/neo4j/conf
      - neo4j_data:/data
      - neo4j_import:/import
      - neo4j_plugins:/plugins
    environment:
      # Raise memory limits
      - NEO4J_server.memory.heap.initial_size=2G
      - NEO4J_server.memory.heap.max_size=4G
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*
    healthcheck:
      test: ["CMD" ,"wget", "http://localhost:7474"]
      interval: 1m30s
      timeout: 10s
      retries: 2
      start_period: 40s
    networks:
      - development
      - monitoring
    <<: *logging

  pgvector:
    image: ankane/pgvector
    ports:
      - 45432:5432
    env_file:
      ./.env.pgvector
    volumes:
      - pgvector_data:/var/lib/postgresql/data
    healthcheck:
      test: pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    networks:
      - development
      - monitoring
    <<: *logging

  api:
    build:
      context: ../../api
      dockerfile: Dockerfile
      tags:
        - api:${VERSION:-latest}
    restart: unless-stopped
    environment:
      PORT: $PORT
    ports:
      - $HOST:$PORT:$PORT
    env_file:
      - ./.env.proc
    depends_on:
      mongodb-dev:
        condition: service_healthy
      neo4j-dev:
        condition: service_healthy
      rabbitmq-dev:
        condition: service_healthy
      redis-api:
        condition: service_healthy
    networks:
      - development
      - monitoring
    <<: *logging

  redis-api:
    image: redis:7.0.11
    restart: unless-stopped
    command: [ "redis-server", "/usr/local/etc/redis/redis.conf" ]
    volumes:
      - type: bind
        source: ./redis/api.conf
        target: /usr/local/etc/redis/redis.conf
        read_only: true
    healthcheck:
      test: [ "CMD", "redis-cli", "--raw", "incr", "ping" ]
      interval: 60s
      timeout: 10s
      retries: 2
      start_period: 40s
    networks:
      - development
      - monitoring
    <<: *logging

  tc-discord-bot-dev:
    build:
      context: ../../tc-discordBot
      target: prod
      dockerfile: Dockerfile
      tags:
        - tc-discord-bot:${VERSION:-latest}
    restart: unless-stopped
    env_file:
      - ./.env.bot.proc
    depends_on:
      mongodb-dev:
        condition: service_healthy
      redis-discord-bot-dev:
        condition: service_healthy
      rabbitmq-dev:
        condition: service_healthy
    networks:
      - development
      - monitoring
    <<: *logging

  discord-analyzer-server:
    image: ghcr.io/togethercrew/discord-analyzer:pr-60
    command: python3 server.py
    restart: unless-stopped
    env_file:
      - ./.env.analyzer.proc
    depends_on:
      redis-discord-analyzer-dev:
        condition: service_healthy
      rabbitmq-dev:
        condition: service_healthy
    networks:
      - development
      - monitoring
    <<: *logging

  discord-analyzer-worker:
    image: ghcr.io/togethercrew/discord-analyzer:pr-60
    command: python3 worker.py
    restart: unless-stopped
    env_file:
      - ./.env.analyzer.proc
    depends_on:
      mongodb-dev:
        condition: service_healthy
      redis-discord-analyzer-dev:
        condition: service_healthy
      neo4j-dev:
        condition: service_healthy
      rabbitmq-dev:
        condition: service_healthy
    networks:
      - development
      - monitoring
    <<: *logging

  # redis-twitter-bot:
  #   image: redis:7.0.11
  #   restart: unless-stopped
  #   command: [ "redis-server", "/usr/local/etc/redis/redis.conf" ]
  #   volumes:
  #     - type: bind
  #       source: ./redis/twitter.conf
  #       target: /usr/local/etc/redis/redis.conf
  #       read_only: true
  #   healthcheck:
  #     test: [ "CMD", "redis-cli", "--raw", "incr", "ping" ]
  #     interval: 60s
  #     timeout: 10s
  #     retries: 2
  #     start_period: 40s
  #   networks:
  #     - development
  #     - monitoring
  #   <<: *logging

  # twitter-bot-server:
  #   image: ghcr.io/togethercrew/twitter-bot:main
  #   command: python3 server.py
  #   restart: unless-stopped
  #   env_file:
  #     - ./.env.twitter
  #   depends_on:
  #     mongodb-dev:
  #       condition: service_healthy
  #     neo4j-dev:
  #       condition: service_healthy
  #     rabbitmq-dev: 
  #       condition: service_healthy
  #     redis-twitter-bot:
  #       condition: service_healthy
  #   networks:
  #     - development
  #     - monitoring
  #   <<: *logging

  # twitter-bot-worker:
  #   image: ghcr.io/togethercrew/twitter-bot:main
  #   command: python3 worker.py
  #   restart: unless-stopped
  #   env_file:
  #     - ./.env.twitter
  #   depends_on:
  #     mongodb-dev:
  #       condition: service_healthy
  #     neo4j-dev:
  #       condition: service_healthy
  #     rabbitmq-dev: 
  #       condition: service_healthy
  #     redis-twitter-bot:
  #       condition: service_healthy
  #   networks:
  #     - development
  #     - monitoring
  #   <<: *logging

  # HIVEMIND SERVICES - START
  
  # HIVEMIND SERVICES - END

  # DISCOURSE SERVICES - START

  discourse-redis:
    image: redis:7.0.11
    restart: unless-stopped
    command: [ "redis-server", "/usr/local/etc/redis/redis.conf" ]
    volumes:
      - type: bind
        source: ./redis/discourse.conf
        target: /usr/local/etc/redis/redis.conf
        read_only: true
    healthcheck:
      test: [ "CMD", "redis-cli", "--raw", "incr", "ping" ]
      interval: 60s
      timeout: 10s
      retries: 2
      start_period: 40s
    networks:
      - discourse
      - monitoring
    <<: *logging

  discourse:
    image: ghcr.io/togethercrew/discourse:main
    command: yarn run start:prod
    restart: always
    ports:
      - 43001:3000
    env_file:
      - .env.discourse
    depends_on:
      discourse-redis:
        condition: service_healthy
      rabbitmq-dev:
        condition: service_healthy
      neo4j-dev:
        condition: service_healthy
    networks:
      - discourse
      - development
      - monitoring
    <<: *logging

  # DISCOURSE SERVICES - END

  # AIRFLOW SERVICES - START

  airflow-redis:
    <<: *redis-common
    volumes:
      - ./redis/airflow.conf:/usr/local/etc/redis/redis.conf:ro
    networks:
      - development
      - monitoring

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - 48080:8080
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      # yamllint disable rule:line-length
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command: -c "./init.sh"
    env_file:
      - ./.env.airflow
      - ./.env.airflow.init
    user: "0:0"
    volumes:
      - airflow_sources:/sources

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      CONNECTION_CHECK_MAX_COUNT: "0"
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command: bash -c airflow

  # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up
  # or by explicitly targeted on the command line e.g. docker-compose up flower.
  # See: https://docs.docker.com/compose/profiles/
  flower:
    <<: *airflow-common
    command: celery flower
    profiles:
      - flower
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  hivemind-server:
    <<: *hivemind-common

  hivemind-worker:
    <<: *hivemind-common
    command: python3 worker.py

  # AIRFLOW SERVICES - END

  # MONITORING SERVICES - START

  grafana:
    image: grafana/grafana
    container_name: grafana
    restart: always
    volumes:
      - ./grafana/provisioning/:/etc/grafana/provisioning
      - grafana_volume:/var/lib/grafana
    depends_on:
      - prometheus
      - loki
    ports:
      - 3000:3000
    networks:
      - monitoring
    <<: *logging

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    command:
      - --config.file=/etc/prometheus/prometheus.yml
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_volume:/prometheus
    depends_on:
      - cadvisor
      - node-exporter
    networks:
      - monitoring
    restart: unless-stopped
    <<: *logging

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    depends_on:
      - redis-cadvisor
    networks:
      - monitoring
    <<: *logging

  redis-cadvisor:
    image: redis:latest
    container_name: redis-cadvisor
    networks:
      - monitoring
    <<: *logging

  loki:
    container_name: loki
    image: grafana/loki:2.8.0
    restart: unless-stopped
    ports:
      - 3100
    volumes:
      - ./loki/loki-config.yaml:/etc/loki/loki-config.yaml
      - loki_volume:/data/loki
    command: -config.file=/etc/loki/loki-config.yaml
    networks:
      monitoring:
        ipv4_address: 172.22.22.15
    <<: *logging

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - monitoring
    <<: *logging

  tempo:
    image: grafana/tempo:latest
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - ./tempo/tempo.yaml:/etc/tempo.yaml
      - ./tempo-data/:/tmp/tempo
    ports:
      - "14268:14268"  # jaeger ingest
      - "3200:3200"   # tempo
      - "9095:9095" # tempo grpc
      - "4317:4317"  # otlp grpc
      - "4318:4318"  # otlp http
      - "9411:9411"   # zipkin
    networks:
      - monitoring
      - development
    <<: *logging

  #MONITORING SERVICES - END

volumes:
  mongodb_data_container:
  rmq_data_container:
  grafana_volume:
  graphite_volume:
  prometheus_volume:
  loki_volume:
  neo4j_data:
  neo4j_import:
  neo4j_plugins:
  hivemind_vector_store:
  pgvector_data:
  airflow_config:
  airflow_logs:
  airflow_plugins:
  airflow_sources:

networks:
  development:
    driver: bridge
  hivemind:
    driver: bridge
  discourse:
    driver: bridge
  monitoring:
    driver: bridge
    ipam:
      config:
        - subnet: 172.22.22.0/16